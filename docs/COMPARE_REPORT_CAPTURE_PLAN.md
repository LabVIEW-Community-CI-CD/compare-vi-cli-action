<!-- markdownlint-disable-next-line MD041 -->
# LVCompare Capture HTML Integration (Phase 1)

The standing-priority issue tracks the DX work to surface compare outcomes deterministically.  
This note captures the Phase&nbsp;1 inventory and requirements for pulling the
`lvcompare-capture.json` artefact (and related LVCompare outputs) into the
HTML report generated by `scripts/Render-CompareReport.ps1`.

## Current data flow

- **Entry points**  
  - `tools/TestStand-CompareHarness.ps1` orchestrates warmup → `Invoke-LVCompare.ps1` and
    stages compare artefacts under `<OutputRoot>/compare/`.  
  - Direct callers can also use `tools/Invoke-LVCompare.ps1`, which resolves LabVIEW/LVCompare
    paths, normalises flags, and invokes `scripts/Capture-LVCompare.ps1`.
- **Capture stage (`scripts/Capture-LVCompare.ps1`)**  
  - Launches `LVCompare.exe`, records stdout/stderr/exit code, and writes
    `lvcompare-capture.json` (schema `lvcompare-capture-v1`) alongside text logs. When
    `-RenderReport` is set the HTML report now lives under `_staging/compare/compare-report.html`
    inside the selected output directory.  
  - Produces additional breadcrumbs (`lvcompare-path.txt`, stopwatch timing) consumed in tests.
- **Report stage (`scripts/Render-CompareReport.ps1`)**  
  - Builds the HTML dashboard today from command metadata, compare-exec JSON, anomalies,
    process snapshots, and additional compare summaries.  
  - Collects a best-effort artefact list (`compare-exec.json`, `lvcompare-capture.json`,
    summary files) for display but does not yet read the capture payload.
- **Telemetry / downstream consumption**  
  - `tools/Invoke-LVCompare.ps1` writes NDJSON events (`prime-lvcompare-v1`) and optional
    leak summaries, and propagates exit code / duration for summaries.  
  - Workflows and docs reference the capture file in `PARAMS_AND_OUTPUTS.md`; no schema-lite
    validation or provenance wiring is currently enforced for the capture payload.

## Desired HTML experience

- Present an explicit **Compare Outcome** card sourced from `lvcompare-capture.json`
  (exit code, diff boolean, elapsed seconds).
- Surface **command and argument details** with copy-to-clipboard affordances, showing both
  normalised flags and the resolved CLI path.
- Provide **stdout/stderr previews** (first _N_ lines/bytes, with download links). Indicate when
  logs are truncated to keep the report lightweight.
- Deep-link to the raw artefacts (capture JSON, compare logs, LVCompare report) in the existing
  Artefacts section, highlighting missing files with reason badges.
- Retain the current detective signals (content diff, anomaly badges) but annotate when
  capture metadata disagrees (e.g., exit ≠ 0/1).

## Constraints & “longer wire” considerations

- Capture files can grow if stdout/stderr are embedded; plan for size caps (e.g., 16&nbsp;KB inline,
  remainder available via download link).  
- HTML rendering must remain deterministic and non-interactive in CI (no live fetches).  
- Provenance updates: extend summary/provenance emitters so the run metadata lists the capture
  artefact path and exit code source.  
- Workflows currently upload compare artefacts implicitly (via directory globs); confirm that the
  capture JSON, report HTML, and raw logs are included for orchestrated runs to avoid broken links.
- Schema coverage: add a Zod schema for `lvcompare-capture-v1` and wire it into the generator so
  schema-lite checks can run in Phase&nbsp;2.

## Open questions heading into Phase 2

1. Should we persist additional capture metadata (LVCompare version, machine info) to enrich the
   HTML dashboard?
2. Do we need user-configurable truncation thresholds or are fixed limits acceptable?
3. How should we surface error exit codes (>1) — distinct badge, summary banner, or both?
4. Can the existing compare anomaly detection reuse capture data instead of re-computing hashes?

Phase&nbsp;2 will prototype the HTML embedding, schema updates, and provenance wiring based on this
inventory.

## Phase 2 snapshot

- `lvcompare-capture-v1` now ships with the schema generator and is validated via the new
  `tests/ParseCompareExec.Tests.ps1` suite.
- `tools/Parse-CompareExec.ps1` prefers capture metadata, emits stdout/stderr byte counts, and
  annotates `compare-outcome.json` plus step summaries with capture/report paths (falling back to
  `compare-exec.json` when necessary).
- `tools/Invoke-LVCompare.ps1 -Summary` expands the compare outcome block with capture hints,
  stdout/stderr byte totals, and artifact locations to align with the summary acceptance criteria.

